---
title: "EWS_BUICK_VECM_PROD"
author: "David A. Bignotti"
date: "March 5, 2019"
output: 
  html_document: 
    df_print: tibble
    keep_md: yes
---

#Setup & Load Data
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plotly)
library(vars)
library(dplyr)
library(plyr)
library(tidyr)
library(DT)
library(forecast)
library(tseries)
library(tsDyn)
library(rlang)
library(lubridate)
library(RColorBrewer)
library(readr)
library(data.table)
library(timetk)
library(lpirfs)
library(TSstudio)
library(sparsevar)
library(ggpubr)
library(gridExtra)
library(dygraphs)
library(gtools)
library(ggfan)
library(ggfortify)
```

```{r import data, include=FALSE}
data <- read_csv("C:/Users/USER/Documents/WORK/Projects/Project_Name/data.csv", 
                 col_types = cols(date = col_date(format = "%m/%d/%Y")))
```

#Diagnostic Testing
```{r diagnostic tests, eval=FALSE, include=FALSE}
#IMPORTANT: as new data is added to the time series, the result of the following diagnostic tests may change. Therefore, run each test one by one and update the associated RESULT comment next to it with the latest test result. If there is no change, then make no changes to the RESULT comment

#Augmented Dickey-Fuller Test: tests for the null hypothesis of ADF is d = 0 against the alternative hypothesis of d < 0. If we do not reject the null hypothesis, then the series is non-stationary whereas rejection means the series is stationary.
adf.variable1 <- tseries::adf.test(data$variable1)
adf.variable2 <- tseries::adf.test(data$variable2)
adf.variable3 <- tseries::adf.test(data$variable3)
adf.variable4 <- tseries::adf.test(data$variable4)
adf.sales <- tseries::adf.test(data$sales)
adf.variable1 #RESULT: reject null hypothesis (series is stationary at a significance level of a = 0.05)
adf.variable2 #RESULT: accept null hypothesis; series is I(1)
adf.variable3 #RESULT: accept null hypothesis; series is I(1)
adf.variable4 #RESULT: accept null hypothesis; series is I(1)
adf.sales #RESULT: reject null hypothesis

#KPSS Test: tests for the null hypothesis that a time series is stationary whereas the alternative hypothesis is non-stationarity
kpss.variable1 <- tseries::kpss.test(data$variable1, null = "Trend")
kpss.variable2 <- tseries::kpss.test(data$variable2, null = "Trend")
kpss.variable3 <- tseries::kpss.test(data$variable3, null = "Trend")
kpss.variable4 <- tseries::kpss.test(data$variable4, null = "Trend")
kpss.sales <- tseries::kpss.test(data$sales, null = "Trend")
kpss.variable1 #RESULT: accept null hypothesis
kpss.variable2 #RESULT: accept null hypothesis
kpss.variable3 #RESULT: reject null hypothesis
kpss.variable4 #RESULT: reject null hypothesis
kpss.sales #RESULT: accept null hypothesis

#Phillips-Perron Test: tests for the null hypothesis that a time series is non-stationary
ppt.variable1 <- tseries::pp.test(data$variable1, alternative = "stationary")
ppt.variable2 <- tseries::pp.test(data$variable2, alternative = "stationary")
ppt.variable3 <- tseries::pp.test(data$variable3, alternative = "stationary")
ppt.variable4 <- tseries::pp.test(data$variable4, alternative = "stationary")
ppt.sales <- tseries::pp.test(data$sales, alternative = "stationary")
ppt.variable1 #RESULT: reject null hypothesis
ppt.variable2 #RESULT: reject null hypothesis
ppt.variable3 #RESULT: accept null hypothesis
ppt.variable4 #RESULT: reject null hypothesis
ppt.sales #RESULT: reject null hypothesis

# rm(adf.variable1)
# rm(adf.variable2)
# rm(adf.variable4)
# rm(adf.sales)
```

```{r transform data, include=FALSE}
#Keep a raw data version
raw_data <- data[1:47,]
raw_data <- raw_data %>%
  dplyr::select(-variable3)

# Log-transform the data
log_data <- data %>%
  mutate(ln.variable1 = log(variable1), ln.variable2 = log(variable2), ln.variable4 = log(variable4), ln.sales = log(sales)) %>%
  select(-variable1, -variable2, -variable3, -variable4, -sales)

log_data <- log_data[1:47,]
```

```{r plot time series}
#Plot the ln(x) time series data
data.frame(time = data$date, data$variable1) %>% 
  melt(id = "time") %>% 
  ggplot(aes(x = time, y = value, colour = variable)) +
  geom_line(size = 1, alpha = 0.5) + 
  ggtitle("variable1") 

data.frame(time = data$date, data$variable2) %>% 
  melt(id = "time") %>% 
  ggplot(aes(x = time, y = value, colour = variable)) +
  geom_line(size = 1, alpha = 0.5) + 
  ggtitle("variable2") 

data.frame(time = data$date, data$variable3) %>%
  melt(id = "time") %>%
  ggplot(aes(x = time, y = value, colour = variable)) +
  geom_line(size = 1, alpha = 0.5) +
  ggtitle("variable3")

data.frame(time = data$date, data$variable4) %>% 
  melt(id = "time") %>% 
  ggplot(aes(x = time, y = value, colour = variable)) +
  geom_line(size = 1, alpha = 0.5) + 
  ggtitle("variable4") 

data.frame(time = data$date, data$sales) %>% 
  melt(id = "time") %>% 
  ggplot(aes(x = time, y = value, colour = variable)) +
  geom_line(size = 1, alpha = 0.5) + 
  ggtitle("Sales") 

```

#Parameter Selection
```{r determine optimal lag and cointegration}
# We fit data to a VAR to obtain the optimal VAR length. Use SC information criterion to find optimal model.
varest <- dplyr::select(raw_data, variable1, variable2, variable4, sales) %>%
  VAR(p = 1, type = "both", lag.max = 6, ic = "AIC")

#Obtain lag length of VAR that best fits the data
laglength <- max(2, varest$p)
laglength <- 3

#Perform Johansen procedure for cointegration
#Allow intercepts in the cointegrating vector: data without zero mean
#Use trace statistic (null hypothesis: number of cointegrating vectors <= r)
res <- dplyr::select(raw_data, variable1, variable2, variable4, sales) %>%
  ca.jo(type = "trace", ecdet = "const", K = laglength, spec = "transitory")

testStatistics <- res@teststat
criticalValues <- res@cval

summary(res) #select cointegration coefficient r for which test value < 10pct level value

r <- 3
```

#Model Implementation
```{r VECM}
VECmodel <- dplyr::select(raw_data, variable1, variable2, variable4, sales) %>%
  VECM(lag = laglength, r = r, include = "both", estim = "ML") #lag=2, r=2 produced lowest error rate

summary(VECmodel)
AIC(VECmodel)
```

#Forecast Implementation
```{r VEC forecast plots, include=FALSE}
Vec2Var <- vec2var(res, r = r)
predictions <- predict(Vec2Var, n.ahead = 12, ci = 0.8)

plot(predictions, names = "variable1")
plot(predictions, names = "variable2")
plot(predictions, names = "variable4")
plot(predictions, names = "sales")
```

```{r de-transform forecast data, include=FALSE}
vec.predict <- predict(VECmodel, n.ahead = 12)
vec.predict <- data.frame(vec.predict)

# vec.predict <- vec.predict %>%
#   mutate(variable1 = exp(ln.variable1), variable2 = exp(ln.variable2), variable4 = exp(ln.variable4), sales = exp(ln.sales)) %>%
#   select(-ln.variable1, -ln.variable2, -ln.variable4, -ln.sales)

write.csv(vec.predict, "C:/Users/USER/Documents/WORK/Projects/Project_Name/forecast/vecm_forecast.csv")
```

```{r VEC IRFs, fig.height=5, fig.width=8}
plot(irf(VECmodel, n.ahead = 12, ci = 0.9, ortho = TRUE, impulse = c("variable1", "variable2", "variable4"), response = "sales"))
```

```{r VEC FEVDs, fig.height=8, fig.width=8}
plot(fevd(VECmodel), col = brewer.pal(n = 5, name = "Set1"))
```

```{r VEC fanchart, fig.height=8, fig.width=10}
fanchart(predictions, colors = brewer.pal(n = 5, name = "Blues"), nc = 2)
```

